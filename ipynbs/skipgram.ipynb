{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training word vectors using Skip Gram with negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/sanika/miniconda3/envs/smai/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home2/sanika/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home2/sanika/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "# import scipy sparse matrix to use scipy.sparse.linalg.svds\n",
    "import scipy.sparse as sp\n",
    "import scipy.sparse.linalg as linalg\n",
    "from tqdm import tqdm\n",
    "from preprocess import Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load textual data from csv file as a list of strings\n",
    "def load_data(file_path):\n",
    "    # load only the second column of the csv file\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            data.append(row[1])\n",
    "    # delete the first element of the list (header)\n",
    "    del data[0]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data('./ANLP-2/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_data, vocab, tokenized_data = Preprocess(data, True, 1)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make pairs of positive and negative samples\n",
    "def make_pairs(data, window_size):\n",
    "    pairs = []\n",
    "    for sentence in tqdm(data):\n",
    "        for i in range(len(sentence)):\n",
    "            for j in range(i - window_size, i + window_size + 1):\n",
    "                if j < 0 or j >= len(sentence) or i == j:\n",
    "                    continue\n",
    "                pairs.append((sentence[i], sentence[j]))\n",
    "    return pairs\n",
    "\n",
    "def make_negative_pairs(data, vocab, num_negative_samples):\n",
    "    pairs = []\n",
    "    for sentence in tqdm(data):\n",
    "        for i in range(len(sentence)):\n",
    "            for j in range(num_negative_samples):\n",
    "                rand_index = np.random.randint(len(vocab))\n",
    "                pairs.append((sentence[i], rand_index))\n",
    "    return pairs\n",
    "\n",
    "# concatenate and add output labels\n",
    "def make_dataset(data, vocab, window_size, num_negative_samples):\n",
    "    positive_pairs = make_pairs(data, window_size)\n",
    "    # remove duplicates\n",
    "    positive_pairs = list(set(positive_pairs))\n",
    "    negative_pairs = make_negative_pairs(data, vocab, num_negative_samples)\n",
    "    # remove duplicates\n",
    "    negative_pairs = list(set(negative_pairs))\n",
    "    # remove pairs from negative pairs that are in positive pairs\n",
    "    dataset = {}\n",
    "    for pair in positive_pairs:\n",
    "        dataset[pair] = 1\n",
    "    for pair in tqdm(negative_pairs):\n",
    "        if pair not in dataset:\n",
    "            dataset[pair] = 0\n",
    "\n",
    "    # pick negative pairs out (value = 0)\n",
    "    negative_pairs = []\n",
    "    for pair in dataset:\n",
    "        if dataset[pair] == 0:\n",
    "            negative_pairs.append(pair)\n",
    "\n",
    "    # randomly sample negative pairs so that the number of negative pairs is equal to the number of positive pairs\n",
    "    # get indices of negative pairs\n",
    "    indices = np.random.choice(len(negative_pairs), len(positive_pairs)*3, replace=False)\n",
    "    negative_pairs = [negative_pairs[i] for i in indices]\n",
    "    \n",
    "    return positive_pairs, negative_pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3892/120000 [00:00<00:06, 19140.32it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120000/120000 [00:05<00:00, 20862.52it/s]\n",
      "100%|██████████| 120000/120000 [01:09<00:00, 1736.21it/s]\n",
      "100%|██████████| 13027893/13027893 [00:09<00:00, 1405263.01it/s]\n"
     ]
    }
   ],
   "source": [
    "positive_pairs, negative_pairs = make_dataset(indexed_data, vocab, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3235938"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(positive_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset and dataloader\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, positive_pairs, negative_pairs):\n",
    "        self.positive_pairs = positive_pairs\n",
    "        self.negative_pairs = negative_pairs\n",
    "        self.pairs = self.positive_pairs + self.negative_pairs\n",
    "        self.labels = [1 for _ in range(len(self.positive_pairs))] + [0 for _ in range(len(self.negative_pairs))]\n",
    "        # convert all the pairs and labels to tensors\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.pairs[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(positive_pairs, negative_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the dataset\n",
    "loaders = {\n",
    "    'train': torch.utils.data.DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay so till now i have the indexed data, i have the vocab and the tokenized data\n",
    "# now, for getting embeddings using skipgram, i need to make pairs of negative and positive samples\n",
    "\n",
    "class Skip_Gram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Skip_Gram, self).__init__()\n",
    "        self.target_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.context_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc = nn.Linear(embedding_dim * 2, embedding_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(embedding_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, target, context):\n",
    "        target_embed = self.target_embedding(target)\n",
    "        context_embed = self.context_embedding(context)\n",
    "        # concatenate the embeddings\n",
    "        embed = torch.cat((target_embed, context_embed), 1)\n",
    "        out = self.fc(embed)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "    def fit(self, loaders, optimizer, criterion, n_epochs, device):\n",
    "        self.to(device)\n",
    "        self.train()\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            self.train()\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            for i, (data, labels) in enumerate(loaders['train']):\n",
    "                optimizer.zero_grad()\n",
    "                # convert to tensors\n",
    "                target = data[0]\n",
    "                context = data[1]\n",
    "                target = target.to(device)\n",
    "                context = context.to(device)\n",
    "                output = self(target, context)\n",
    "                # squeeze the output\n",
    "                output = output.squeeze()\n",
    "                # convert to float\n",
    "                labels = labels.to(device, dtype=torch.float32)\n",
    "                loss = criterion(output, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                correct += torch.sum((output > 0.5) == labels).item()\n",
    "            print(f'Epoch: {epoch+1}/{n_epochs}, Loss: {total_loss/len(loaders[\"train\"])}')\n",
    "            print(f'Accuracy: {correct/len(loaders[\"train\"].dataset)}')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5, Loss: 0.317218183780918\n",
      "Accuracy: 0.8692236995888055\n",
      "Epoch: 2/5, Loss: 0.30238448707185345\n",
      "Accuracy: 0.8745051666626493\n",
      "Epoch: 3/5, Loss: 0.296065924534819\n",
      "Accuracy: 0.8776670010364847\n",
      "Epoch: 4/5, Loss: 0.2891443773657926\n",
      "Accuracy: 0.8811608875077335\n",
      "Epoch: 5/5, Loss: 0.2832462140417006\n",
      "Accuracy: 0.8838778740507389\n"
     ]
    }
   ],
   "source": [
    "model = Skip_Gram(len(vocab), 300)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "n_epochs = 5\n",
    "\n",
    "model.fit(loaders, optimizer, criterion, n_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save target embeddings as a pickle file \n",
    "embeddings_dict = {}\n",
    "for idx, embedding in enumerate(model.target_embedding.weight):\n",
    "    embeddings_dict[idx] = embedding.to('cpu').detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as pt\n",
    "torch.save(embeddings_dict, 'skip-gram-vocab.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save vocab\n",
    "torch.save(vocab, 'skip-gram-vocab.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
