{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training word vectors using SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/sanika/miniconda3/envs/smai/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home2/sanika/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "# import scipy sparse matrix to use scipy.sparse.linalg.svds\n",
    "import scipy.sparse as sp\n",
    "import scipy.sparse.linalg as linalg\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load textual data from csv file as a list of strings\n",
    "def load_data(file_path):\n",
    "    # load only the second column of the csv file\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            data.append(row[1])\n",
    "    # delete the first element of the list (header)\n",
    "    del data[0]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data('./ANLP-2/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess():\n",
    "    def __init__ (self, data, train=True):\n",
    "        self.data = data\n",
    "        self.train = train\n",
    "\n",
    "    def tokenize(self, data):\n",
    "        tokenized_data = []\n",
    "        for text in data:\n",
    "            text = re.sub(r'\\\\', ' ', text)\n",
    "            text = re.sub(r'\\\"', ' ', text)\n",
    "            text = re.sub(r'\\d+', '<NUMBER>', text)\n",
    "            text = text.lower()\n",
    "            # remove punctuation\n",
    "            tokenized_data.append(word_tokenize(text))\n",
    "        return tokenized_data\n",
    "\n",
    "    def convert_to_outofvocab(self, data):\n",
    "        # make a dictionary of frequencies, words that appear less than 3 times are converted to <OOV>\n",
    "        freq_dict = {}\n",
    "        for sentence in data:\n",
    "            for word in sentence:\n",
    "                if word in freq_dict:\n",
    "                    freq_dict[word] += 1\n",
    "                else:\n",
    "                    freq_dict[word] = 1\n",
    "\n",
    "        for i in range(len(data)):\n",
    "            for j in range(len(data[i])):\n",
    "                if freq_dict[data[i][j]] < 2:\n",
    "                    data[i][j] = '<OOV>'\n",
    "        return data\n",
    "    \n",
    "    def build_vocab(self, tokenized_data):\n",
    "        OUT_OF_VOCAB = '<OOV>'\n",
    "        PAD_TAG = '<PAD>'\n",
    "        START_TAG = '<BOS>'\n",
    "        END_TAG = '<EOS>'\n",
    "        vocab = build_vocab_from_iterator(tokenized_data, specials=[OUT_OF_VOCAB, PAD_TAG, START_TAG, END_TAG])\n",
    "        return vocab\n",
    "    \n",
    "    def text_to_indices(self, tokenized_data, vocab):\n",
    "        indexed_data = []\n",
    "        for sentence in tokenized_data:\n",
    "            indexed_data.append([vocab[token] for token in sentence])\n",
    "        return indexed_data\n",
    "    \n",
    "    def __call__(self):\n",
    "        tokenized_data = self.tokenize(self.data)\n",
    "        if self.train:\n",
    "            tokenized_data = self.convert_to_outofvocab(tokenized_data)\n",
    "        vocab = self.build_vocab(tokenized_data)\n",
    "        indexed_data = self.text_to_indices(tokenized_data, vocab)\n",
    "        return indexed_data, vocab, tokenized_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_data, vocab, tokenized_data = Preprocess(data, True)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a co-occurrence matrix\n",
    "def build_co_occurrence_matrix(corpus, vocab, window_size=5):\n",
    "    co_occurrence_matrix = sp.lil_matrix((len(vocab), len(vocab)), dtype=np.float32)\n",
    "    for sentence in tqdm(corpus):\n",
    "        for i, word in enumerate(sentence):\n",
    "            for j in range(max(i - window_size, 0), min(i + window_size + 1, len(sentence))):\n",
    "                if i != j:\n",
    "                    co_occurrence_matrix[word, sentence[j]] += 1\n",
    "    return co_occurrence_matrix\n",
    "\n",
    "def perform_svd(co_occurrence_matrix, k=300):\n",
    "    U, S, V = linalg.svds(co_occurrence_matrix, k=k)\n",
    "    word_vectors = get_word_vectors(U, S, k)\n",
    "    return word_vectors\n",
    "\n",
    "# Select word vectors\n",
    "def get_word_vectors(U, S, k):\n",
    "    word_vectors = U[:, :k]\n",
    "    return word_vectors\n",
    "\n",
    "# Map words to their corresponding word vectors\n",
    "def map_words_to_vectors(word_vectors, word_to_index):\n",
    "    word_to_vector = {}\n",
    "    for word, index in word_to_index.items():\n",
    "        word_to_vector[word] = word_vectors[index]\n",
    "    return word_to_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33981"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120000/120000 [03:07<00:00, 639.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cooccurence matrix built\n",
      "Word vectors obtained\n"
     ]
    }
   ],
   "source": [
    "coocerrence_matrix = build_co_occurrence_matrix(indexed_data, vocab)\n",
    "print(\"Cooccurence matrix built\")\n",
    "word_vectors = perform_svd(coocerrence_matrix)\n",
    "print(\"Word vectors obtained\")\n",
    "word_to_vector = map_words_to_vectors(word_vectors, vocab.get_stoi())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save word vectors with the index of the word in the vocab as a dictionary (pickle file)\n",
    "import pickle\n",
    "with open('word_vectors_2.pkl', 'wb') as f:\n",
    "    pickle.dump(word_vectors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the vocabulary as a pickle file\n",
    "import pickle\n",
    "\n",
    "with open('vocab_2.pkl', 'wb') as file:\n",
    "    pickle.dump(vocab, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the word vector into a text file\n",
    "with open('word_vectors.txt', 'w') as file:\n",
    "    for i in range(len(word_vectors)):\n",
    "        file.write(f'{i} ')\n",
    "        for j in range(len(word_vectors[i])):\n",
    "            file.write(f'{word_vectors[i][j]} ')\n",
    "        file.write('\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
